<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>HDFS and MapReduce: Revolutionizing Big Data Processing | Renny Harlin</title>
<meta name="keywords" content="">
<meta name="description" content="HDFS and MapReduce can be confusing at times. Let&rsquo;s break down the entire process step-by-step with a concrete example.
The example I&rsquo;m going to use is calculating the average movie rating per genre from a CSV file. First, we have to upload the CSV file into HDFS, and then we will run a MapReduce job to compute the average ratings.
We assume:


Data format:
genre,rating


Goal: average rating per genre">
<meta name="author" content="Renny Harlin">
<link rel="canonical" href="https://rennyharlin.github.io/posts/big-data/map-reduce/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fd5566c526ae48aadabd950798a2dc3568536401560eae5caac3765a42a9e7b5.css" integrity="sha256-/VVmxSauSKravZUHmKLcNWhTZAFWDq5cqsN2WkKp57U=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://rennyharlin.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="https://rennyharlin.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="32x32" href="https://rennyharlin.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="apple-touch-icon" href="https://rennyharlin.github.io/%3Clink%20/%20abs%20url%3E">
<link rel="mask-icon" href="https://rennyharlin.github.io/%3Clink%20/%20abs%20url%3E">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://rennyharlin.github.io/posts/big-data/map-reduce/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>

      <script async src="https://www.googletagmanager.com/gtag/js?id=G-Y8J11SHRX5"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-Y8J11SHRX5');
        }
      </script><meta property="og:url" content="https://rennyharlin.github.io/posts/big-data/map-reduce/">
  <meta property="og:site_name" content="Renny Harlin">
  <meta property="og:title" content="HDFS and MapReduce: Revolutionizing Big Data Processing">
  <meta property="og:description" content="HDFS and MapReduce can be confusing at times. Let’s break down the entire process step-by-step with a concrete example.
The example I’m going to use is calculating the average movie rating per genre from a CSV file. First, we have to upload the CSV file into HDFS, and then we will run a MapReduce job to compute the average ratings.
We assume:
Data format:
genre,rating
Goal: average rating per genre">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-12-23T17:43:33+05:30">
    <meta property="article:modified_time" content="2025-12-23T17:43:33+05:30">
      <meta property="og:image" content="https://rennyharlin.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://rennyharlin.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E">
<meta name="twitter:title" content="HDFS and MapReduce: Revolutionizing Big Data Processing">
<meta name="twitter:description" content="HDFS and MapReduce can be confusing at times. Let&rsquo;s break down the entire process step-by-step with a concrete example.
The example I&rsquo;m going to use is calculating the average movie rating per genre from a CSV file. First, we have to upload the CSV file into HDFS, and then we will run a MapReduce job to compute the average ratings.
We assume:


Data format:
genre,rating


Goal: average rating per genre">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://rennyharlin.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "HDFS and MapReduce: Revolutionizing Big Data Processing",
      "item": "https://rennyharlin.github.io/posts/big-data/map-reduce/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "HDFS and MapReduce: Revolutionizing Big Data Processing",
  "name": "HDFS and MapReduce: Revolutionizing Big Data Processing",
  "description": "HDFS and MapReduce can be confusing at times. Let\u0026rsquo;s break down the entire process step-by-step with a concrete example.\nThe example I\u0026rsquo;m going to use is calculating the average movie rating per genre from a CSV file. First, we have to upload the CSV file into HDFS, and then we will run a MapReduce job to compute the average ratings.\nWe assume:\nData format:\ngenre,rating\nGoal: average rating per genre\n",
  "keywords": [
    
  ],
  "articleBody": "HDFS and MapReduce can be confusing at times. Let’s break down the entire process step-by-step with a concrete example.\nThe example I’m going to use is calculating the average movie rating per genre from a CSV file. First, we have to upload the CSV file into HDFS, and then we will run a MapReduce job to compute the average ratings.\nWe assume:\nData format:\ngenre,rating\nGoal: average rating per genre\nEnvironment: Hadoop (HDFS + YARN + MapReduce)\nSTEP 0 — Your local file (before Hadoop) On your local machine you have a file:\ngenre_ratings.csv Contents:\nDrama,4.5\rComedy,3.0\rDrama,5.0\rSci-Fi,4.8\rComedy,3.5 At this point:\nHadoop is not involved This is a normal OS file STEP 1 — Uploading the CSV into HDFS You now copy the file into HDFS.\nCommand hdfs dfs -mkdir -p /movies/genre_ratings hdfs dfs -put genre_ratings.csv /movies/genre_ratings/ What happens internally (very important) Client contacts NameNode\nNameNode:\nChecks permissions Chooses DataNodes to store blocks File is split into HDFS blocks\nDefault block size = 128 MB Your file is small → 1 block Block is:\nWritten to 3 DataNodes (replication factor = 3) Different views of the file:\nUser view of HDFS: /movies/genre_ratings/genre_ratings.csv NameNode view of file (Name Node metadata) ratings.csv\rblock_1 → DN2, DN4, DN7\rblock_2 → DN1, DN3, DN6 DataNode view of blocks DN2: blk_1\rDN4: blk_1\rDN7: blk_1\rDN1: blk_2\rDN3: blk_2\rDN6: blk_2 Data is now distributed and fault-tolerant.\nSTEP 2 — Submitting the MapReduce job You submit a MapReduce job (e.g., Hadoop Streaming).\nExample command hadoop jar hadoop-streaming.jar \\ -input /movies/genre_ratings/genre_ratings.csv \\ -output /movies/output_avg \\ -mapper mapper.py \\ -reducer reducer.py What Hadoop does now Job is submitted to YARN ResourceManager\nResourceManager:\nAllocates containers\nLaunches an ApplicationMaster\nApplicationMaster:\nRequests block locations from the NameNode\nDetermines number of map tasks\nData Locality Optimization NameNode knows where blocks reside ApplicationMaster requests containers on the same nodes Mapper reads data locally from the DataNode disk Each mapper runs:\nIn its own YARN container As a separate JVM process One mapper per InputSplit STEP 3 — InputSplits and Map task creation (data locality) InputSplit creation File size \u003c 128 MB 1 block → 1 InputSplit → 1 mapper If file were TBs:\nThousands of blocks Thousands of mappers For every InputSplit,\nOne mapper is created. YARN tries to schedule mappers on DataNodes where data resides (data locality). STEP 4 — Mapper execution (line-by-line) What a mapper receives Each mapper receives:\n(key, value) Example:\n(0, \"Drama,4.5\")\r(10, \"Comedy,3.0\") key = byte offset (ignored) value = one line from CSV Mapper logic (conceptual) Read one line Split by comma Convert rating to float Emit genre as key Mapper output (logical) Drama → (4.5,1)\rComedy → (3.0,1)\rDrama → (5.0,1)\rSci-Fi → (4.8,1)\rComedy → (3.5,1) Mapper output (physical reality) Output is buffered in memory\nWhen buffer fills:\nData spilled to disk Sorted by key (genre) Example spill file:\nComedy (3.0,1)\rComedy (3.5,1)\rDrama (4.5,1)\rDrama (5.0,1)\rSci-Fi (4.8,1) STEP 5 — Combiner (optional but realistic) Before shuffle:\nCombiner runs on mapper node Performs local aggregation After combiner Comedy → (6.5,2)\rDrama → (9.5,2)\rSci-Fi → (4.8,1) This drastically reduces network traffic.\nSTEP 6 — Shuffle and Sort phase This is the heart of MapReduce. This is the most expensive phase in the job, because it involves data transfer over the network.\nWhat happens Mapper outputs are:\nPartitioned by key Sent over the network All records with same genre go to same reducer (partitioner decides)\nReducer receives data already sorted\nExample reducer input:\nComedy → [(6.5,2)]\rDrama → [(9.5,2)]\rSci-Fi → [(4.8,1)] STEP 7 — Reducer execution Reducer logic For each genre:\nSum all partial sums Sum all counts Compute average Reducer computation Comedy:\rsum = 6.5\rcount = 2\ravg = 3.25\rDrama:\rsum = 9.5\rcount = 2\ravg = 4.75\rSci-Fi:\rsum = 4.8\rcount = 1\ravg = 4.8 Reducer output Comedy 3.25\rDrama 4.75\rSci-Fi 4.8 STEP 8 — Writing output to HDFS Reducer writes output to HDFS\nStored as:\n/movies/output_avg/part-00000 Block replicated across DataNodes\nJob marked SUCCESS\nSTEP 9 — Viewing the result Command hdfs dfs -cat /movies/output_avg/part-00000 Output Comedy 3.25\rDrama 4.75\rSci-Fi 4.8 STEP 10 — Job completion \u0026 cleanup YARN releases containers Temporary files removed Logs stored for debugging FULL PIPELINE Local CSV\r↓\rHDFS upload (blocks + replication)\r↓\rInputSplit creation\r↓\rMapper (parse, emit genre → rating)\r↓\rCombiner (local aggregation)\r↓\rShuffle \u0026 Sort\r↓\rReducer (sum, count, avg)\r↓\rOutput written to HDFS ",
  "wordCount" : "744",
  "inLanguage": "en",
  "image": "https://rennyharlin.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished": "2025-12-23T17:43:33+05:30",
  "dateModified": "2025-12-23T17:43:33+05:30",
  "author":{
    "@type": "Person",
    "name": "Renny Harlin"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://rennyharlin.github.io/posts/big-data/map-reduce/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Renny Harlin",
    "logo": {
      "@type": "ImageObject",
      "url": "https://rennyharlin.github.io/%3Clink%20/%20abs%20url%3E"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://rennyharlin.github.io/" accesskey="h" title="Renny Harlin (Alt + H)">Renny Harlin</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://rennyharlin.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://rennyharlin.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://rennyharlin.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      HDFS and MapReduce: Revolutionizing Big Data Processing
    </h1>
    <div class="post-meta"><span title='2025-12-23 17:43:33 +0530 IST'>December 23, 2025</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>Renny Harlin</span>&nbsp;|&nbsp;<span>
    <a href="https://github.com/RennyHarlin/RennyHarlin.github.io/blob/main/content/posts/big-data/map-reduce.md" rel="noopener noreferrer edit" target="_blank">Suggest Changes</a>
</span>

</div>
  </header> 
  <div class="post-content"><p>HDFS and MapReduce can be confusing at times. Let&rsquo;s break down the entire process step-by-step with a concrete example.</p>
<p>The example I&rsquo;m going to use is calculating the average movie rating per genre from a CSV file. First, we have to upload the CSV file into HDFS, and then we will run a MapReduce job to compute the average ratings.</p>
<p>We assume:</p>
<ul>
<li>
<p>Data format:</p>
<p><code>genre,rating</code></p>
</li>
<li>
<p>Goal: <strong>average rating per genre</strong></p>
</li>
<li>
<p>Environment: Hadoop (HDFS + YARN + MapReduce)</p>
</li>
</ul>
<hr>
<h1 id="step-0--your-local-file-before-hadoop">STEP 0 — Your local file (before Hadoop)<a hidden class="anchor" aria-hidden="true" href="#step-0--your-local-file-before-hadoop">#</a></h1>
<p>On your local machine you have a file:</p>
<pre tabindex="0"><code>genre_ratings.csv
</code></pre><p>Contents:</p>
<pre tabindex="0"><code>Drama,4.5
Comedy,3.0
Drama,5.0
Sci-Fi,4.8
Comedy,3.5
</code></pre><p>At this point:</p>
<ul>
<li>Hadoop is <strong>not involved</strong></li>
<li>This is a normal OS file</li>
</ul>
<hr>
<h1 id="step-1--uploading-the-csv-into-hdfs">STEP 1 — Uploading the CSV into HDFS<a hidden class="anchor" aria-hidden="true" href="#step-1--uploading-the-csv-into-hdfs">#</a></h1>
<p>You now copy the file into HDFS.</p>
<h3 id="command">Command<a hidden class="anchor" aria-hidden="true" href="#command">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>hdfs dfs -mkdir -p /movies/genre_ratings
</span></span><span style="display:flex;"><span>hdfs dfs -put genre_ratings.csv /movies/genre_ratings/
</span></span></code></pre></div><hr>
<h2 id="what-happens-internally-very-important">What happens internally (very important)<a hidden class="anchor" aria-hidden="true" href="#what-happens-internally-very-important">#</a></h2>
<ol>
<li>
<p><strong>Client contacts NameNode</strong></p>
</li>
<li>
<p>NameNode:</p>
<ul>
<li>Checks permissions</li>
<li>Chooses DataNodes to store blocks</li>
</ul>
</li>
<li>
<p>File is split into <strong>HDFS blocks</strong></p>
<ul>
<li>Default block size = <strong>128 MB</strong></li>
<li>Your file is small → <strong>1 block</strong></li>
</ul>
</li>
<li>
<p>Block is:</p>
<ul>
<li>Written to <strong>3 DataNodes</strong> (replication factor = 3)</li>
</ul>
</li>
<li>
<p>Different views of the file:</p>
<ul>
<li>User view of HDFS:</li>
</ul>
<pre tabindex="0"><code>/movies/genre_ratings/genre_ratings.csv
</code></pre><ul>
<li>NameNode view of file (Name Node metadata)</li>
</ul>
<pre tabindex="0"><code> ratings.csv
     block_1 → DN2, DN4, DN7
     block_2 → DN1, DN3, DN6
</code></pre><ul>
<li>DataNode view of blocks</li>
</ul>
<pre tabindex="0"><code> DN2: blk_1
 DN4: blk_1
 DN7: blk_1
 DN1: blk_2
 DN3: blk_2
 DN6: blk_2
</code></pre></li>
</ol>
<p>Data is now <strong>distributed and fault-tolerant</strong>.</p>
<hr>
<h1 id="step-2--submitting-the-mapreduce-job">STEP 2 — Submitting the MapReduce job<a hidden class="anchor" aria-hidden="true" href="#step-2--submitting-the-mapreduce-job">#</a></h1>
<p>You submit a MapReduce job (e.g., Hadoop Streaming).</p>
<h3 id="example-command">Example command<a hidden class="anchor" aria-hidden="true" href="#example-command">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>hadoop jar hadoop-streaming.jar <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -input /movies/genre_ratings/genre_ratings.csv <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -output /movies/output_avg <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -mapper mapper.py <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  -reducer reducer.py
</span></span></code></pre></div><hr>
<h3 id="what-hadoop-does-now">What Hadoop does now<a hidden class="anchor" aria-hidden="true" href="#what-hadoop-does-now">#</a></h3>
<p>Job is submitted to YARN ResourceManager</p>
<ol>
<li>
<p>ResourceManager:</p>
<ul>
<li>
<p>Allocates containers</p>
</li>
<li>
<p>Launches an ApplicationMaster</p>
</li>
</ul>
</li>
<li>
<p>ApplicationMaster:</p>
<ul>
<li>
<p>Requests block locations from the NameNode</p>
</li>
<li>
<p>Determines number of map tasks</p>
</li>
</ul>
</li>
</ol>
<h3 id="data-locality-optimization">Data Locality Optimization<a hidden class="anchor" aria-hidden="true" href="#data-locality-optimization">#</a></h3>
<ul>
<li>NameNode knows where blocks reside</li>
<li>ApplicationMaster requests containers on the same nodes</li>
<li>Mapper reads data locally from the DataNode disk</li>
</ul>
<p>Each mapper runs:</p>
<ul>
<li>In its own YARN container</li>
<li>As a separate JVM process</li>
<li>One mapper per InputSplit</li>
</ul>
<hr>
<h1 id="step-3--inputsplits-and-map-task-creation-data-locality">STEP 3 — InputSplits and Map task creation (data locality)<a hidden class="anchor" aria-hidden="true" href="#step-3--inputsplits-and-map-task-creation-data-locality">#</a></h1>
<h3 id="inputsplit-creation">InputSplit creation<a hidden class="anchor" aria-hidden="true" href="#inputsplit-creation">#</a></h3>
<ul>
<li>File size &lt; 128 MB</li>
<li><strong>1 block → 1 InputSplit → 1 mapper</strong></li>
</ul>
<p>If file were TBs:</p>
<ul>
<li>Thousands of blocks</li>
<li>Thousands of mappers</li>
</ul>
<p>For every InputSplit,</p>
<ul>
<li>One mapper is created.</li>
<li>YARN tries to schedule mappers on DataNodes where data resides (data locality).</li>
</ul>
<hr>
<h1 id="step-4--mapper-execution-line-by-line">STEP 4 — Mapper execution (line-by-line)<a hidden class="anchor" aria-hidden="true" href="#step-4--mapper-execution-line-by-line">#</a></h1>
<h3 id="what-a-mapper-receives">What a mapper receives<a hidden class="anchor" aria-hidden="true" href="#what-a-mapper-receives">#</a></h3>
<p>Each mapper receives:</p>
<pre tabindex="0"><code>(key, value)
</code></pre><p>Example:</p>
<pre tabindex="0"><code>(0, &#34;Drama,4.5&#34;)
(10, &#34;Comedy,3.0&#34;)
</code></pre><ul>
<li><code>key</code> = byte offset (ignored)</li>
<li><code>value</code> = one line from CSV</li>
</ul>
<hr>
<h2 id="mapper-logic-conceptual">Mapper logic (conceptual)<a hidden class="anchor" aria-hidden="true" href="#mapper-logic-conceptual">#</a></h2>
<ol>
<li>Read one line</li>
<li>Split by comma</li>
<li>Convert rating to float</li>
<li>Emit genre as key</li>
</ol>
<h3 id="mapper-output-logical">Mapper output (logical)<a hidden class="anchor" aria-hidden="true" href="#mapper-output-logical">#</a></h3>
<pre tabindex="0"><code>Drama  → (4.5,1)
Comedy → (3.0,1)
Drama  → (5.0,1)
Sci-Fi → (4.8,1)
Comedy → (3.5,1)
</code></pre><hr>
<h2 id="mapper-output-physical-reality">Mapper output (physical reality)<a hidden class="anchor" aria-hidden="true" href="#mapper-output-physical-reality">#</a></h2>
<ul>
<li>
<p>Output is <strong>buffered in memory</strong></p>
</li>
<li>
<p>When buffer fills:</p>
<ul>
<li>Data spilled to disk</li>
<li>Sorted by key (genre)</li>
</ul>
</li>
</ul>
<p>Example spill file:</p>
<pre tabindex="0"><code>Comedy  (3.0,1)
Comedy  (3.5,1)
Drama   (4.5,1)
Drama   (5.0,1)
Sci-Fi  (4.8,1)
</code></pre><hr>
<h1 id="step-5--combiner-optional-but-realistic">STEP 5 — Combiner (optional but realistic)<a hidden class="anchor" aria-hidden="true" href="#step-5--combiner-optional-but-realistic">#</a></h1>
<p>Before shuffle:</p>
<ul>
<li>Combiner runs <strong>on mapper node</strong></li>
<li>Performs local aggregation</li>
</ul>
<h3 id="after-combiner">After combiner<a hidden class="anchor" aria-hidden="true" href="#after-combiner">#</a></h3>
<pre tabindex="0"><code>Comedy → (6.5,2)
Drama  → (9.5,2)
Sci-Fi → (4.8,1)
</code></pre><p>This <strong>drastically reduces network traffic</strong>.</p>
<hr>
<h1 id="step-6--shuffle-and-sort-phase">STEP 6 — Shuffle and Sort phase<a hidden class="anchor" aria-hidden="true" href="#step-6--shuffle-and-sort-phase">#</a></h1>
<p>This is the <strong>heart of MapReduce</strong>. This is the most expensive phase in the job, because it involves <strong>data transfer over the network</strong>.</p>
<h3 id="what-happens">What happens<a hidden class="anchor" aria-hidden="true" href="#what-happens">#</a></h3>
<ol>
<li>
<p>Mapper outputs are:</p>
<ul>
<li>Partitioned by key</li>
<li>Sent over the network</li>
</ul>
</li>
<li>
<p>All records with same genre go to <strong>same reducer</strong> (partitioner decides)</p>
</li>
<li>
<p>Reducer receives data <strong>already sorted</strong></p>
</li>
</ol>
<p>Example reducer input:</p>
<pre tabindex="0"><code>Comedy → [(6.5,2)]
Drama  → [(9.5,2)]
Sci-Fi → [(4.8,1)]
</code></pre><hr>
<h1 id="step-7--reducer-execution">STEP 7 — Reducer execution<a hidden class="anchor" aria-hidden="true" href="#step-7--reducer-execution">#</a></h1>
<h3 id="reducer-logic">Reducer logic<a hidden class="anchor" aria-hidden="true" href="#reducer-logic">#</a></h3>
<p>For each genre:</p>
<ol>
<li>Sum all partial sums</li>
<li>Sum all counts</li>
<li>Compute average</li>
</ol>
<h3 id="reducer-computation">Reducer computation<a hidden class="anchor" aria-hidden="true" href="#reducer-computation">#</a></h3>
<pre tabindex="0"><code>Comedy:
  sum = 6.5
  count = 2
  avg = 3.25

Drama:
  sum = 9.5
  count = 2
  avg = 4.75

Sci-Fi:
  sum = 4.8
  count = 1
  avg = 4.8
</code></pre><hr>
<h2 id="reducer-output">Reducer output<a hidden class="anchor" aria-hidden="true" href="#reducer-output">#</a></h2>
<pre tabindex="0"><code>Comedy   3.25
Drama    4.75
Sci-Fi   4.8
</code></pre><hr>
<h1 id="step-8--writing-output-to-hdfs">STEP 8 — Writing output to HDFS<a hidden class="anchor" aria-hidden="true" href="#step-8--writing-output-to-hdfs">#</a></h1>
<ol>
<li>
<p>Reducer writes output to HDFS</p>
</li>
<li>
<p>Stored as:</p>
<pre tabindex="0"><code>/movies/output_avg/part-00000
</code></pre></li>
<li>
<p>Block replicated across DataNodes</p>
</li>
<li>
<p>Job marked <strong>SUCCESS</strong></p>
</li>
</ol>
<hr>
<h1 id="step-9--viewing-the-result">STEP 9 — Viewing the result<a hidden class="anchor" aria-hidden="true" href="#step-9--viewing-the-result">#</a></h1>
<h3 id="command-1">Command<a hidden class="anchor" aria-hidden="true" href="#command-1">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>hdfs dfs -cat /movies/output_avg/part-00000
</span></span></code></pre></div><h3 id="output">Output<a hidden class="anchor" aria-hidden="true" href="#output">#</a></h3>
<pre tabindex="0"><code>Comedy   3.25
Drama    4.75
Sci-Fi   4.8
</code></pre><hr>
<h1 id="step-10--job-completion--cleanup">STEP 10 — Job completion &amp; cleanup<a hidden class="anchor" aria-hidden="true" href="#step-10--job-completion--cleanup">#</a></h1>
<ul>
<li>YARN releases containers</li>
<li>Temporary files removed</li>
<li>Logs stored for debugging</li>
</ul>
<hr>
<h1 id="full-pipeline">FULL PIPELINE<a hidden class="anchor" aria-hidden="true" href="#full-pipeline">#</a></h1>
<pre tabindex="0"><code>Local CSV
   ↓
HDFS upload (blocks + replication)
   ↓
InputSplit creation
   ↓
Mapper (parse, emit genre → rating)
   ↓
Combiner (local aggregation)
   ↓
Shuffle &amp; Sort
   ↓
Reducer (sum, count, avg)
   ↓
Output written to HDFS
</code></pre>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="next" href="https://rennyharlin.github.io/posts/computer-science/regular-expressions/">
    <span class="title">Next »</span>
    <br>
    <span>Regular Expressions</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share HDFS and MapReduce: Revolutionizing Big Data Processing on x"
            href="https://x.com/intent/tweet/?text=HDFS%20and%20MapReduce%3a%20Revolutionizing%20Big%20Data%20Processing&amp;url=https%3a%2f%2frennyharlin.github.io%2fposts%2fbig-data%2fmap-reduce%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share HDFS and MapReduce: Revolutionizing Big Data Processing on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frennyharlin.github.io%2fposts%2fbig-data%2fmap-reduce%2f&amp;title=HDFS%20and%20MapReduce%3a%20Revolutionizing%20Big%20Data%20Processing&amp;summary=HDFS%20and%20MapReduce%3a%20Revolutionizing%20Big%20Data%20Processing&amp;source=https%3a%2f%2frennyharlin.github.io%2fposts%2fbig-data%2fmap-reduce%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share HDFS and MapReduce: Revolutionizing Big Data Processing on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2frennyharlin.github.io%2fposts%2fbig-data%2fmap-reduce%2f&title=HDFS%20and%20MapReduce%3a%20Revolutionizing%20Big%20Data%20Processing">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share HDFS and MapReduce: Revolutionizing Big Data Processing on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frennyharlin.github.io%2fposts%2fbig-data%2fmap-reduce%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share HDFS and MapReduce: Revolutionizing Big Data Processing on whatsapp"
            href="https://api.whatsapp.com/send?text=HDFS%20and%20MapReduce%3a%20Revolutionizing%20Big%20Data%20Processing%20-%20https%3a%2f%2frennyharlin.github.io%2fposts%2fbig-data%2fmap-reduce%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share HDFS and MapReduce: Revolutionizing Big Data Processing on telegram"
            href="https://telegram.me/share/url?text=HDFS%20and%20MapReduce%3a%20Revolutionizing%20Big%20Data%20Processing&amp;url=https%3a%2f%2frennyharlin.github.io%2fposts%2fbig-data%2fmap-reduce%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share HDFS and MapReduce: Revolutionizing Big Data Processing on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=HDFS%20and%20MapReduce%3a%20Revolutionizing%20Big%20Data%20Processing&u=https%3a%2f%2frennyharlin.github.io%2fposts%2fbig-data%2fmap-reduce%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://rennyharlin.github.io/">Renny Harlin</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
